{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Between the Lines: Deciphering IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, we will perform sentiment analysis on the IMDb dataset of 50k movie reviews. Sentiment analysis, also known as opinion mining, uses natural language processing (NLP), text analysis, and computational linguistics to identify and extract subjective information from source materials.\n",
    "\n",
    "## Here's an overview of the analytical process:\n",
    "\n",
    "Data Loading and Preprocessing: The first step in text analysis involves loading the dataset into a suitable data structure. The IMDb dataset typically consists of movie reviews (text data) and their corresponding sentiment labels (positive or negative). After the data is loaded, preprocessing steps such as normalization, tokenization, stemming/lemmatization, stop-word removal, and others are applied to clean the data and prepare it for further analysis.\n",
    "\n",
    "**Feature Extraction:** Once the data is cleaned, we convert the text data into numerical features that can be used as inputs to machine learning algorithms. Techniques like Bag of Words, TF-IDF, or more advanced methods like word embeddings (Word2Vec, GloVe) or transformers can be used at this step.\n",
    "\n",
    "**Model Building:** After feature extraction, we build a machine learning or deep learning model that can classify movie reviews as positive or negative based on the input features. Models commonly used for text-based classification include Naive Bayes, SVM, Random Forests, Logistic Regression, Neural Networks, or even more complex models such as Transformer-based.\n",
    "\n",
    "**Model Training:** Once the model architecture is defined, it is trained using the features from the movie reviews and their corresponding sentiment labels. The trained model can be evaluated on a separate test set to determine how well it generalizes to unseen data. Evaluation metrics commonly used include accuracy, precision, recall, F1-score, and/or AUC-ROC.\n",
    "\n",
    "**Optimization and Tuning:** Depending on the evaluation results, the model might be further optimized or tuned by optimizing parameters, changing the model architecture, or using different feature extraction techniques. This process might be repeated until satisfactory results are achieved.\n",
    "These are the steps we will take to analyze IMDb movie reviews. \n",
    "\n",
    "Let's begin by loading the necessary tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for working with data loaded as data-frame\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "import re # re regular expressions module, we will use it for text cleaning tasks such as removing  punctuation, etc..\n",
    "import nltk # Natural Language Toolkit\n",
    "\n",
    "#nltk.download('punkt') # for tokenization\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: data exploration and cleaning;\n",
    "\n",
    "The first step involves loading the IMDb dataset, which consists of movie reviews (text data) and their corresponding sentiment labels (positive or negative), into a suitable data structure. After the data is loaded, it is crucial to inspect what the data has to offer. This helps us determine the necessary preprocessing steps to clean the data and prepare it for further analysis,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv('/Users/.../IMDB_Dataset.csv') # load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head(10) # after loading it is a good ideat to look at e.g. 10 top entries or so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the loading of our dataset, we can employ the *info()* function from pandas to generate a concise summary of our DataFrame. This function reveals vital information about the dataset including the total number of rows and columns, the count of non-null values in each column, the data types in each column, and the memory usage of the DataFrame. Understanding these details is crucial as they guide subsequent steps in the data cleaning and preprocessing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data frame contains non-numerical data, *describe()* comand can provide us useful statistics,such as:\n",
    "\n",
    "**count** - Number of non-null (or non-NA) observations.\n",
    "\n",
    "**unique** -  Number of distinct values.\n",
    "\n",
    "**top** - Most frequent value.\n",
    "\n",
    "**freq** - Frequency of the most frequent value.\n",
    "\n",
    "Thus by using describe(include='all') will provide us statistical summaries for both numerical and non-numerical columns as is evident in cell given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  negative\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon analyzing the IMDb movie reviews dataset, we've identified 49,582 unique entries out of a total of 50,000. Consequently, our next step involves locating and eliminating any duplicate entries. We can accomplish this using the *drop_duplicates()* function provided by pandas. This function considers all columns by default and removes all rows that are identical across these columns, leaving only the first occurrence. The command that implements this is as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = imdb_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we have successfully retained only unique entries for subsequent analysis, we can verify the total number of rows remaining in our dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if our IMDb dataset is balanced with respect to the sentiment, we can use the *value_counts()* function available in pandas. This function counts the number of occurrences of each unique value in a single column of our data-frame, in this case, the 'sentiment' column.\n",
    "\n",
    "**Note:** A perfectly balanced dataset would have exactly the same number of positive and negative reviews. However, in practice, a slight imbalance is typically not a problem. But, if there is a large imbalance, we might need to consider using attitional techniques such as upsampling the minority class, downsampling the majority class to deal with this issue. This is important for many machine learning algorithms, as a serious imbalance can make the algorithm biased towards the majority class, thereby affecting the performance of the model.\n",
    "\n",
    "To check this in our current dataset, we type,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    24884\n",
       "negative    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the number of positive and negative reviews is relatively small (186 reviews), this indicates that our dataset is \"roughly\" balanced (i.e. we have approximately the same number of 'positive' and 'negative' instances).\n",
    "\n",
    " Given that the class distribution in our case is almost equal, we should be able to proceed with model building without having to worry about handling class imbalance. It is a good practice to always check the class distribution before starting the model building process!\n",
    "\n",
    "We can also visualise this by plotting the respective counts of positive and negative sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"white\") # no background colour\n",
    "deep_colors = sns.color_palette(\"deep\") # we will use colours from deep palette\n",
    "custom_palette =[deep_colors[2], deep_colors[3]] # pre-select red and green from deep palette \n",
    "sns.countplot(x='sentiment', data=imdb_data, palette = custom_palette)\n",
    "plt.title(\"Sentiment distribution\") # add title to plot\n",
    "sns.despine() # to remove the top and right borders of the plot.\n",
    "plt.savefig('/Users/zurauskj/Desktop/FellowshipAI/Analysis_Plots/sentiment_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a single review before applying text cleaning procedures,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.iloc[8]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean/normalise text-based data using combination of string methods and regular expressions (from the re module). For simplicity, we define a single function to carry out the task and apply it on our data-frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    \n",
    "    return text\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves that the text was processed as desired we re-examin the same entry by printing the review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encouraged by the positive comments about this film on here i was looking forward to watching this film bad mistake ive seen 950 films and this is truly one of the worst of them  its awful in almost every way editing pacing storyline acting soundtrack the films only song  a lame country tune  is played no less than four times the film looks cheap and nasty and is boring in the extreme rarely have i been so happy to see the end credits of a film the only thing that prevents me giving this a 1score is harvey keitel  while this is far from his best performance he at least seems to be making a bit of an effort one for keitel obsessives only\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.iloc[8]['review'])\n",
    "#imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next good step after cleaning text data is to tokenize it. This process involves breaking down the text into individual words/tokens. Tokenization is important because it breaks text into meaningful units (tokens), which is easier for the machine learning models to understand and process. We can achieve this by using *nltk* command *word_tokenize*,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['review'] = imdb_data['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how it worked out on same review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encouraged', 'by', 'the', 'positive', 'comments', 'about', 'this', 'film', 'on', 'here', 'i', 'was', 'looking', 'forward', 'to', 'watching', 'this', 'film', 'bad', 'mistake', 'ive', 'seen', '950', 'films', 'and', 'this', 'is', 'truly', 'one', 'of', 'the', 'worst', 'of', 'them', 'its', 'awful', 'in', 'almost', 'every', 'way', 'editing', 'pacing', 'storyline', 'acting', 'soundtrack', 'the', 'films', 'only', 'song', 'a', 'lame', 'country', 'tune', 'is', 'played', 'no', 'less', 'than', 'four', 'times', 'the', 'film', 'looks', 'cheap', 'and', 'nasty', 'and', 'is', 'boring', 'in', 'the', 'extreme', 'rarely', 'have', 'i', 'been', 'so', 'happy', 'to', 'see', 'the', 'end', 'credits', 'of', 'a', 'film', 'the', 'only', 'thing', 'that', 'prevents', 'me', 'giving', 'this', 'a', '1score', 'is', 'harvey', 'keitel', 'while', 'this', 'is', 'far', 'from', 'his', 'best', 'performance', 'he', 'at', 'least', 'seems', 'to', 'be', 'making', 'a', 'bit', 'of', 'an', 'effort', 'one', 'for', 'keitel', 'obsessives', 'only']\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.iloc[8]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform stopwords removal. Stopwords are common words like 'is', 'the', 'and', etc.; it is a good idea to remove them as they do not carry much meaningful information for the context (thus for the model too). We remove them using *nltk*'s list of English stopwords,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    filtered_text = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_stopwords) # Apply the function to the 'review' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how it worked out on same review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encouraged', 'positive', 'comments', 'film', 'looking', 'forward', 'watching', 'film', 'bad', 'mistake', 'ive', 'seen', '950', 'films', 'truly', 'one', 'worst', 'awful', 'almost', 'every', 'way', 'editing', 'pacing', 'storyline', 'acting', 'soundtrack', 'films', 'song', 'lame', 'country', 'tune', 'played', 'less', 'four', 'times', 'film', 'looks', 'cheap', 'nasty', 'boring', 'extreme', 'rarely', 'happy', 'see', 'end', 'credits', 'film', 'thing', 'prevents', 'giving', '1score', 'harvey', 'keitel', 'far', 'best', 'performance', 'least', 'seems', 'making', 'bit', 'effort', 'one', 'keitel', 'obsessives']\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.iloc[8]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be stemming/lemmatization; we will choose between these based on what it does, thus,\n",
    "\n",
    "**Stemming:** usually refers to a crude heuristic process that chops off the ends of words. Stemmers use language-specific rules, but they often have no understanding of the context, and thus cannot handle irregular cases correctly. For example, the stemmer might not be able to correctly stem the word \"better\" to \"good\", and would simply remove the \"er\" to get \"bett\".\n",
    "\n",
    "**Lemmatization:** takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. For example, \"better\" should be correctly lemmatized to \"good\".\n",
    "\n",
    "In summary, stemming is considered faster as it simply trims off the end of the word, while lemmatization is slower but more accurate as it needs to look up the lemma in a dictionary. For this dataset, lemmatization is better choice, as we favour acuracy over speed. Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].apply(lemmatize_text) # Apply the function to the 'review' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let us check how it worked using same review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encouraged positive comment film looking forward watching film bad mistake ive seen 950 film truly one worst awful almost every way editing pacing storyline acting soundtrack film song lame country tune played le four time film look cheap nasty boring extreme rarely happy see end credit film thing prevents giving 1score harvey keitel far best performance least seems making bit effort one keitel obsessive\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data.iloc[8]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature extraction;\n",
    "\n",
    "Now that we have processed IMDb data, it is time to convert text data into a numerical format that can serve later as an input into a machine learning model.\n",
    "\n",
    "We will be using TF-IDF Vectorization to transform lemmatized tokens into a numerical format. TF-IDF approach stands for Term Frequency-Inverse Document Frequency, a measure that reflects how important a word is to a review (\"document\") in the context of the whole reviews. Words that are frequent in a specific review but rare across all reviews are given more weight, while words that are frequent in many reviews are down-weighted.\n",
    "In such way transformed data can then be used as input for machine learning algorithms to perform sentiment analysis, identifying whether each review is positive or negative. Alternative approaches to TF-IDF include, e.g. Bag of Words (BoW) - that represents text data as a matrix of token counts. It counts how many times each word appears in a document; or more advanced approaches exist such as Word2Vec - that involves training a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49582, 210509)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer() #initialize a TfidfVectorizer object\n",
    "X = vectorizer.fit_transform(imdb_data['review'])   # we transform lemmatized reviews into TF-IDF vectors\n",
    "print(X.shape) # check underlying matrix size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, *X* represents a matrix where each row corresponds to a movie review and each column corresponds to a word. The value at the *i*-th row and the *j*-th column contains TF-IDF score of the *j*-th word in the *i*-th review.\n",
    "\n",
    "The TF-IDF score is calculated by mutiplying to scores: TF-IDF(word) = TF(word) * IDF(word), where\n",
    "\n",
    "Term Frequency (TF) -  measures the frequency of a word in a review by calculating the number of times a word appears in the review divided by the total number of words in the same review.\n",
    "\n",
    "Inverse Document Frequency (IDF) -  measures the significance of the word in the whole corpus, which is calculated as the logarithm of the total number of reviews in the dataset divided by the number of reviews containing the word.\n",
    "\n",
    "Higher TF-IDF scores signifies that the word is more important in the document, but it also considers the word's frequency in all reviews. Thus, common words that appear in many reviews will have a lower IDF score and in turn lower TF-IDF score. This helps to balance out the term frequency and not give too much weight to words that appear too frequently. In the context of sentiment analysis, this means words that are more specific and relevant to sentiment ('good', 'amazing', 'dislike', 'terrible', etc.) will be given higher weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment classification;\n",
    "\n",
    "Here we will be performing sentiment classification using Random Forest model. It is a popular choice for text classification tasks, and can work well with high dimensional data like TF-IDF vectors above (recall *X*'s shape above). Briefly, Random Forest(RF) is a type of ensemble machine learning algorithm that is built on decision trees. Ensemble methods, as the name suggests, combine multiple machine learning models to create more powerful models. In the case of RF, the model creates a number of decision trees at training time and outputs the class that is the mode of the classes of the individual trees. For this we will be using RF implementation within *scikit-learn*.\n",
    "\n",
    "We begin by splitting initial data into train and test sets at ratio 80% : 20%, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set is  39665 examples, which is 80 %\n",
      "size of tresting set is  9917 examples, which is 20 %\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, imdb_data['sentiment'], test_size=0.2, random_state=42)\n",
    "print(\"size of training set is \", X_train.shape[0],\"examples, which is\", round(X_train.shape[0]*100/X.shape[0]),\"%\")\n",
    "print(\"size of tresting set is \", X_test.shape[0],\"examples, which is\", round(X_test.shape[0]*100/X.shape[0]),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will initialize a Random Forest model and fit the model using the training data. Then, it will use trained model to make predictions on the test-set and to print a standard classification report, which includes metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.84      0.84      4939\n",
      "    positive       0.84      0.85      0.85      4978\n",
      "\n",
      "    accuracy                           0.84      9917\n",
      "   macro avg       0.84      0.84      0.84      9917\n",
      "weighted avg       0.84      0.84      0.84      9917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42) # Initialize the Random Forest model\n",
    "clf.fit(X_train, y_train) #  Train the model\n",
    "y_pred = clf.predict(X_test)# Make predictions on the test set\n",
    "print(classification_report(y_test, y_pred)) # Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the provided classification report, we can see that model has relatively high performance across all metrics. These results indicate that this model has achieved comparable performance in identifying both positive and negative reviews, with an overall accuracy of 84%. Given that the dataset is almost perfectly balanced (as evidenced by the support), this level of performance suggests a robust model.\n",
    "\n",
    "To seek better perormance, we can explore hyperparameter tuning, which is the process of searching for the optimal hyperparameters for a machine learning algorithm, such as our RF model. The process typically involves specifying a hyperparameter space and a method to search or sample candidates; then the algorithm is trained and validated against a holdout set for each hyperparameter combination.\n",
    "\n",
    "*Scikit-learn* provides several methods for hyperparameter tuning. Below, hyperparameter tuning for a random forest classifier is achieved using GridSearchCV approach,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    ;\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new random forest classifier with the best parameters\n",
    "rf_best = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit and predict\n",
    "rf_best.fit(X_train, y_train)\n",
    "predictions = rf_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.85      0.86      4939\n",
      "    positive       0.85      0.88      0.87      4978\n",
      "\n",
      "    accuracy                           0.86      9917\n",
      "   macro avg       0.86      0.86      0.86      9917\n",
      "weighted avg       0.86      0.86      0.86      9917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, here we have successfully built a text classification model using a Random Forest classifier to predict the sentiment of movie reviews from the IMDb dataset. The preprocessing steps involved cleaning, normalizing the text, tokenization, and vectorization using TF-IDF approach.\n",
    "\n",
    "Initially, the model achieved an accuracy of 84% with precision, recall, and F1-score also around 84% for both classes, showing a balanced performance. This indicates that our model was already performing reasonably well at distinguishing between positive and negative reviews.\n",
    "\n",
    "To further improve the model's performance, the hyperparameter tuning using GridSearchCV was performed, which allowed to systematically work through multiple combinations of parameters, cross-validating as it goes to determine which parameters give the best performance.\n",
    "\n",
    "As a result, the accuracy improved to 86%, and we also saw improvement in precision, recall, and F1-score for both classes. The optimized model shows a balanced performance between both classes, making it a robust model for this task.\n",
    "\n",
    "This demonstrates the power of machine learning for tasks like sentiment analysis and the importance of preprocessing in dealing with natural language analysis. \n",
    "\n",
    "Finally, it is important to note, that sentiment analysis (as given above) is crucial for a number of reasons; e.g. In audience Segmentation, sentiment analysis can help identify different audience segments and their preferences; whereas in context of social research, such analysis can provide insights into societal norms and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
